import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import time
import psutil
import os
from io import BytesIO
import tempfile
from gestalt_ir_image_division import (
    SingleChannelIRDataset,
    OptimizedGestaltIRModel,
    EnhancedGestaltConstraintLoss
)

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ----------------------- CSReconNetæ¨¡å‹ç»„ä»¶ -----------------------
class ChaoticToeplitzSampler(nn.Module):
    def __init__(self, block_size=16, measurement_ratio=0.5):
        super().__init__()
        self.block_size = block_size
        self.measurement_dim = int(block_size**2 * measurement_ratio)
        self.register_buffer('phi', self.generate_01_chaotic_toeplitz())
    
    def generate_logistic_chaotic_sequence(self, length=256, r=3.99):
        x = np.random.rand()
        sequence = []
        for _ in range(length):
            x = r * x * (1 - x)
            sequence.append(x)
        return np.array(sequence)
    
    def generate_chaotic_toeplitz_matrix(self, sequence):
        n = self.block_size**2
        m = self.measurement_dim
        c = sequence[:n]
        r = sequence[1:m+1]
        toeplitz_matrix = np.zeros((m, n))
        for i in range(m):
            for j in range(n):
                k = i - j
                idx = k % len(c) if k >= 0 else (-k) % len(r)
                toeplitz_matrix[i, j] = c[idx] if k >= 0 else r[idx]
        return toeplitz_matrix
    
    def binarize_matrix(self, matrix):
        threshold = np.mean(matrix)
        return (matrix >= threshold).astype(np.float32)
    
    def generate_01_chaotic_toeplitz(self):
        seq = self.generate_logistic_chaotic_sequence()
        toeplitz = self.generate_chaotic_toeplitz_matrix(seq)
        return torch.from_numpy(self.binarize_matrix(toeplitz)).float()
    
    def forward(self, x):
        batch_size = x.size(0)
        x_flat = x.view(batch_size, -1)
        return torch.matmul(x_flat, self.phi.t())

class GaussianSampler(nn.Module):
    def __init__(self, block_size=8, measurement_ratio=0.25):
        super().__init__()
        self.block_size = block_size
        self.measurement_dim = int(block_size**2 * measurement_ratio)
        self.phi = nn.Parameter(torch.randn(self.measurement_dim, block_size**2) * 1/np.sqrt(block_size**2), requires_grad=False)
    
    def forward(self, x):
        batch_size = x.size(0)
        x_flat = x.view(batch_size, -1)
        return torch.matmul(x_flat, self.phi.t())

class BoundaryFusionModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(128, 1, kernel_size=3, padding=1)  # ä¿®æ­£è¾“å…¥é€šé“ä¸º128
    
    def forward(self, x):
        return self.conv(x)

class CSReconNet(nn.Module):
    def __init__(self):
        super().__init__()
        # æµ‹é‡å±‚
        self.important_sampler = ChaoticToeplitzSampler(block_size=16, measurement_ratio=0.4)
        self.non_important_sampler = GaussianSampler(block_size=8, measurement_ratio=0.15)
        
        # é‡è¦åŒºåŸŸå¤„ç†
        self.imp_extractor = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        self.imp_decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 1, kernel_size=3, padding=1)
        )
        
        # éé‡è¦åŒºåŸŸå¤„ç†
        self.non_imp_extractor = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        
        self.non_imp_decoder = nn.Sequential(
            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)
        )
        
        # è¾¹ç•Œèåˆæ¨¡å—
        self.boundary_fusion = BoundaryFusionModule()

    def forward(self, important_blocks, non_important_blocks):
        # ç‰¹å¾æå–
        imp_features = self.imp_extractor(important_blocks) if important_blocks.numel() > 0 else torch.empty(0)
        non_imp_features = self.non_imp_extractor(non_important_blocks) if non_important_blocks.numel() > 0 else torch.empty(0)
        
        # é‡å»ºè¿‡ç¨‹
        x_imp_recon = self.imp_decoder(imp_features)
        x_non_recon = self.non_imp_decoder(non_imp_features)
        
        # è¾¹ç•Œèåˆ
        if x_imp_recon.numel() > 0:
            boundary_info = self.boundary_fusion(imp_features)
            x_imp_recon = x_imp_recon + boundary_info
        
        return x_imp_recon, x_non_recon

# ----------------------- åæ ‡è®¡ç®—å’Œå›¾åƒå¤„ç†å‡½æ•° -----------------------
def get_rotated_bbox_coords(xc, yc, w, h, theta, img_h, img_w):
    """è®¡ç®—æ—‹è½¬åçš„è¾¹ç•Œæ¡†åæ ‡"""
    theta_rad = np.radians(theta)
    cos_theta = np.cos(theta_rad)
    sin_theta = np.sin(theta_rad)

    half_w = w / 2
    half_h = h / 2

    corners = [
        [-half_w, -half_h],
        [half_w, -half_h],
        [half_w, half_h],
        [-half_w, half_h]
    ]

    rotated_corners = []
    for corner in corners:
        x = corner[0] * cos_theta - corner[1] * sin_theta + xc
        y = corner[0] * sin_theta + corner[1] * cos_theta + yc
        rotated_corners.append([x, y])

    min_x = max(0, int(np.min([corner[0] for corner in rotated_corners])))
    max_x = min(img_w, int(np.max([corner[0] for corner in rotated_corners])))
    min_y = max(0, int(np.min([corner[1] for corner in rotated_corners])))
    max_y = min(img_h, int(np.max([corner[1] for corner in rotated_corners])))

    return min_x, min_y, max_x - min_x, max_y - min_y

# åŠ è½½æ ¼å¼å¡”æ¨¡å‹å‡½æ•°
def load_gestalt_model(ckpt_path='Models/optimized_gestalt_model_3.0.pth'):
    """åŠ è½½é¢„è®­ç»ƒçš„æ ¼å¼å¡”åŒºåŸŸåˆ’åˆ†æ¨¡å‹"""
    model = OptimizedGestaltIRModel()
    model.load_state_dict(torch.load(ckpt_path, map_location='cpu'))
    model.eval()
    return model

# å›¾åƒåˆ†å—å‡½æ•°
def image_to_blocks(image, bbox, important_block_size=16, non_important_block_size=8):
    """å°†å›¾åƒæŒ‰è¾¹ç•Œæ¡†åˆ’åˆ†ä¸ºé‡è¦åŒºåŸŸå’Œéé‡è¦åŒºåŸŸå—"""
    # è·å–åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆå‡è®¾ä¸º640x512ï¼‰
    original_width, original_height = 640, 512
    current_h, current_w = image.shape[-2], image.shape[-1]
    
    # ä»å½’ä¸€åŒ–åæ ‡è½¬æ¢å›åŸå§‹å›¾åƒå°ºå¯¸
    bbox = bbox.squeeze().cpu().numpy()
    xc_normalized, yc_normalized, w_box_normalized, h_box_normalized, theta = bbox
    xc_original = xc_normalized * original_width
    yc_original = yc_normalized * original_height
    w_box_original = w_box_normalized * original_width
    h_box_original = h_box_normalized * original_height
    
    # è®¡ç®—åœ¨å½“å‰å¤„ç†å›¾åƒå°ºå¯¸ä¸­çš„åæ ‡
    xc = xc_original * current_w / original_width
    yc = yc_original * current_h / original_height
    w_box = w_box_original * current_w / original_width
    h_box = h_box_original * current_h / original_height

    # è®¡ç®—æ—‹è½¬åçš„è¾¹ç•Œæ¡†åæ ‡
    x1, y1, w_rot, h_rot = get_rotated_bbox_coords(xc, yc, w_box, h_box, theta, current_h, current_w)

    # å¤„ç†è¾¹ç•Œæƒ…å†µ
    x1 = max(0, x1)
    y1 = max(0, y1)
    x2 = min(x1 + w_rot, current_w)
    y2 = min(y1 + h_rot, current_h)

    # é‡è¦åŒºåŸŸå¤„ç†
    important_region = image[..., y1:y2, x1:x2]
    imp_h, imp_w = important_region.shape[-2], important_region.shape[-1]
    
    # è®¡ç®—é‡è¦åŒºåŸŸçš„å—æ•°å’Œåæ ‡
    n_h_imp = max(1, (imp_h + important_block_size - 1) // important_block_size)
    n_w_imp = max(1, (imp_w + important_block_size - 1) // important_block_size)
    
    # é‡è¦åŒºåŸŸå—çš„ä½ç½®ä¿¡æ¯
    important_blocks = []
    important_blocks_pos = []
    
    for i in range(n_h_imp):
        for j in range(n_w_imp):
            block_y_start = i * important_block_size
            block_y_end = min(block_y_start + important_block_size, imp_h)
            block_x_start = j * important_block_size
            block_x_end = min(block_x_start + important_block_size, imp_w)
            
            if block_y_end > block_y_start and block_x_end > block_x_start:
                block = important_region[..., block_y_start:block_y_end, block_x_start:block_x_end]
                
                absolute_y_start = y1 + block_y_start
                absolute_y_end = y1 + block_y_end
                absolute_x_start = x1 + block_x_start
                absolute_x_end = x1 + block_x_end
                
                important_blocks.append(block)
                important_blocks_pos.append({
                    'y_start': absolute_y_start,
                    'y_end': absolute_y_end,
                    'x_start': absolute_x_start,
                    'x_end': absolute_x_end,
                    'h': block_y_end - block_y_start,
                    'w': block_x_end - block_x_start
                })
    
    # è½¬æ¢ä¸ºå¼ é‡
    if important_blocks:
        max_h = max(b.shape[-2] for b in important_blocks)
        max_w = max(b.shape[-1] for b in important_blocks)
        important_blocks_tensor = torch.zeros(len(important_blocks), 1, max_h, max_w, device=image.device)
        
        for i, block in enumerate(important_blocks):
            h, w = block.shape[-2:]
            important_blocks_tensor[i, :, :h, :w] = block
            
        important_blocks = important_blocks_tensor
    else:
        important_blocks = torch.empty(0, 1, important_block_size, important_block_size, device=image.device)
    
    # éé‡è¦åŒºåŸŸå¤„ç†
    non_important_mask = torch.ones_like(image, dtype=bool)
    non_important_mask[..., y1:y2, x1:x2] = False
    non_important_region = image * non_important_mask.float()
    
    # éé‡è¦åŒºåŸŸå—çš„ä½ç½®ä¿¡æ¯
    non_important_blocks = []
    non_important_blocks_pos = []
    
    # å°†éé‡è¦åŒºåŸŸåˆ†å—
    for i in range(0, current_h, non_important_block_size):
        for j in range(0, current_w, non_important_block_size):
            block_y_start = i
            block_y_end = min(i + non_important_block_size, current_h)
            block_x_start = j
            block_x_end = min(j + non_important_block_size, current_w)
            
            if not non_important_mask[..., block_y_start:block_y_end, block_x_start:block_x_end].all():
                continue
                
            if block_y_end > block_y_start and block_x_end > block_x_start:
                block = non_important_region[..., block_y_start:block_y_end, block_x_start:block_x_end]
                
                non_important_blocks.append(block)
                non_important_blocks_pos.append({
                    'y_start': block_y_start,
                    'y_end': block_y_end,
                    'x_start': block_x_start,
                    'x_end': block_x_end,
                    'h': block_y_end - block_y_start,
                    'w': block_x_end - block_x_start
                })
    
    # è½¬æ¢ä¸ºå¼ é‡
    if non_important_blocks:
        max_h = max(b.shape[-2] for b in non_important_blocks)
        max_w = max(b.shape[-1] for b in non_important_blocks)
        non_important_blocks_tensor = torch.zeros(len(non_important_blocks), 1, max_h, max_w, device=image.device)
        
        for i, block in enumerate(non_important_blocks):
            h, w = block.shape[-2:]
            non_important_blocks_tensor[i, :, :h, :w] = block
            
        non_important_blocks = non_important_blocks_tensor
    else:
        non_important_blocks = torch.empty(0, 1, non_important_block_size, non_important_block_size, device=image.device)
    
    return important_blocks, non_important_blocks, (x1, y1, x2, y2), important_blocks_pos, non_important_blocks_pos

# å›¾åƒé‡å»ºå‡½æ•° - ä¿®æ”¹äº†è¾¹ç•Œå¤„ç†å’Œå‹ç¼©ç‡è®¡ç®—
def reconstruct_image(model, gestalt_model, image_tensor):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    gestalt_model = gestalt_model.to(device)
    image_tensor = image_tensor.to(device)
    
    start_time = time.time()
    process = psutil.Process(os.getpid())
    memory_usage = process.memory_info().rss
    
    with torch.no_grad():
        # è·å–è¾¹ç•Œæ¡†
        bbox = gestalt_model(image_tensor)
        
        # å›¾åƒåˆ†å—
        important_blocks, non_important_blocks, bbox_coords, important_blocks_pos, non_important_blocks_pos = image_to_blocks(
            image_tensor, bbox, important_block_size=16, non_important_block_size=8
        )
        
        # è®¡ç®—å®é™…å‹ç¼©ç‡ - åŸºäºæµ‹é‡çŸ©é˜µç»´åº¦
        original_num_elements = image_tensor.numel()
        important_measurements = important_blocks.shape[0] * model.important_sampler.measurement_dim
        non_important_measurements = non_important_blocks.shape[0] * model.non_important_sampler.measurement_dim
        compressed_num_elements = important_measurements + non_important_measurements
        compression_ratio = compressed_num_elements / original_num_elements
        
        # é‡æ„å›¾åƒ
        model.eval()
        if important_blocks.numel() > 0 and non_important_blocks.numel() > 0:
            recon_imp, recon_non = model(important_blocks, non_important_blocks)
        elif important_blocks.numel() > 0:
            recon_imp, _ = model(important_blocks, non_important_blocks[:0])
            recon_non = torch.zeros_like(non_important_blocks)
        else:
            _, recon_non = model(important_blocks[:0], non_important_blocks)
            recon_imp = torch.zeros_like(important_blocks)
        
        # è®¡ç®—é‡æ„æ—¶é—´
        end_time = time.time()
        processing_time = end_time - start_time
        
        # è®¡ç®—MSEæŸå¤±
        mse_imp = torch.mean((recon_imp - important_blocks) ** 2).item() if important_blocks.numel() > 0 else 0
        mse_non = torch.mean((recon_non - non_important_blocks) ** 2).item() if non_important_blocks.numel() > 0 else 0
        
        # æ„å»ºé‡æ„åçš„å®Œæ•´å›¾åƒ
        reconstructed_image = image_tensor.clone()
        x1, y1, x2, y2 = bbox_coords
        
        # é‡æ–°æ„å»ºé‡è¦åŒºåŸŸ - ç§»é™¤äº†é«˜æ–¯æ¨¡ç³Šå¤„ç†
        if important_blocks.numel() > 0 and len(important_blocks_pos) > 0:
            for i, pos in enumerate(important_blocks_pos):
                block = recon_imp[i, :, :pos['h'], :pos['w']]
                reconstructed_image[..., pos['y_start']:pos['y_end'], pos['x_start']:pos['x_end']] = block
        
        # é‡æ–°æ„å»ºéé‡è¦åŒºåŸŸ - ç§»é™¤äº†é«˜æ–¯æ¨¡ç³Šå¤„ç†
        if non_important_blocks.numel() > 0 and len(non_important_blocks_pos) > 0:
            for i, pos in enumerate(non_important_blocks_pos):
                block = recon_non[i, :, :pos['h'], :pos['w']]
                reconstructed_image[..., pos['y_start']:pos['y_end'], pos['x_start']:pos['x_end']] = block
        
        # è®¡ç®—å³°å€¼ä¿¡å™ªæ¯”(PSNR)
        mse = torch.mean((reconstructed_image - image_tensor) ** 2).item()
        psnr = 20 * np.log10(1.0 / np.sqrt(mse)) if mse > 0 else float('inf')
        
        # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIM)
        ssim = calculate_ssim(image_tensor.cpu().numpy(), reconstructed_image.cpu().numpy())
    
    return {
        'original_image': image_tensor.cpu().numpy(),
        'reconstructed_image': reconstructed_image.cpu().numpy(),
        'bbox_coords': bbox_coords,
        'compression_ratio': compression_ratio,
        'processing_time': processing_time,
        'mse_important': mse_imp,
        'mse_non_important': mse_non,
        'memory_usage': memory_usage / (1024 ** 2),
        'psnr': psnr,
        'ssim': ssim,
        'original_num_elements': original_num_elements,
        'compressed_num_elements': compressed_num_elements
    }

# è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°(SSIM)
def calculate_ssim(img1, img2, window_size=11, K1=0.01, K2=0.03, L=1):
    """è®¡ç®—ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„SSIMå€¼"""
    C1 = (K1 * L) ** 2
    C2 = (K2 * L) ** 2
    
    img1 = img1.squeeze()
    img2 = img2.squeeze()
    
    # åˆ›å»ºé«˜æ–¯çª—å£
    window = np.ones((window_size, window_size)) / (window_size * window_size)
    
    # è®¡ç®—å‡å€¼
    mu1 = cv2.filter2D(img1, -1, window)
    mu2 = cv2.filter2D(img2, -1, window)
    
    mu1_sq = mu1 * mu1
    mu2_sq = mu2 * mu2
    mu1_mu2 = mu1 * mu2
    
    # è®¡ç®—æ–¹å·®å’Œåæ–¹å·®
    sigma1_sq = cv2.filter2D(img1 * img1, -1, window) - mu1_sq
    sigma2_sq = cv2.filter2D(img2 * img2, -1, window) - mu2_sq
    sigma12 = cv2.filter2D(img1 * img2, -1, window) - mu1_mu2
    
    # è®¡ç®—SSIM
    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
    
    # è¿”å›å‡å€¼SSIM
    return np.mean(ssim_map)

# ç»˜åˆ¶å›¾åƒå’Œè¾¹ç•Œæ¡†
def plot_image_with_bbox(image, bbox_coords, title, figsize=(8, 8)):
    fig, ax = plt.subplots(figsize=figsize)
    if image.shape[0] == 1:
        ax.imshow(image[0], cmap='gray')
    else:
        ax.imshow(np.transpose(image, (1, 2, 0)))
    
    x1, y1, x2, y2 = bbox_coords
    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, 
                         linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)
    ax.text(x1, y1-10, 'é‡è¦åŒºåŸŸ', color='red', fontsize=12)
    ax.set_title(title)
    ax.axis('off')
    return fig

# ç»˜åˆ¶å›¾åƒå¯¹æ¯”
def plot_image_comparison(original, reconstructed, bbox_coords, title, figsize=(15, 6)):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
    
    # åŸå§‹å›¾åƒ
    original_img = np.squeeze(original)
    ax1.imshow(original_img, cmap='gray')
    
    x1, y1, x2, y2 = bbox_coords
    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, 
                         linewidth=2, edgecolor='r', facecolor='none')
    ax1.add_patch(rect)
    ax1.set_title("åŸå§‹å›¾åƒ")
    ax1.axis('off')
    
    # é‡æ„å›¾åƒ
    reconstructed_img = np.squeeze(original)
    ax2.imshow(reconstructed_img, cmap='gray')
    
    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, 
                         linewidth=2, edgecolor='r', facecolor='none')
    ax2.add_patch(rect)
    ax2.set_title("é‡æ„å›¾åƒ")
    ax2.axis('off')
    
    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    return fig

# ç»˜åˆ¶åƒç´ çº§å¯¹æ¯”å›¾
def plot_pixel_comparison(original, reconstructed, bbox_coords, patch_size=64, figsize=(15, 6)):
    fig, axes = plt.subplots(2, 3, figsize=figsize)
    
    x1, y1, x2, y2 = bbox_coords
    x_center, y_center = (x1 + x2) // 2, (y1 + y2) // 2
    x_patch = max(0, x_center - patch_size // 2)
    y_patch = max(0, y_center - patch_size // 2)
    x_patch_end = min(x_patch + patch_size, original.shape[-1])
    y_patch_end = min(y_patch + patch_size, original.shape[-2])
    
    # åŸå§‹å›¾åƒ
    original_img = np.squeeze(original)
    
    # é‡æ„å›¾åƒ
    reconstructed_img = np.squeeze(reconstructed)
    
    # å…¨å›¾
    axes[0, 0].imshow(original_img, cmap='gray')
    axes[0, 0].set_title("åŸå§‹å›¾åƒ")
    axes[0, 0].axis('off')
    
    axes[1, 0].imshow(original_img, cmap='gray')
    axes[1, 0].set_title("é‡æ„å›¾åƒ")
    axes[1, 0].axis('off')
    
    # å±€éƒ¨æ”¾å¤§
    original_patch = original_img[y_patch:y_patch_end, x_patch:x_patch_end]
    reconstructed_patch = reconstructed_img[y_patch:y_patch_end, x_patch:x_patch_end]
    
    axes[0, 1].imshow(original_patch, cmap='gray')
    axes[0, 1].set_title("åŸå§‹å›¾åƒå±€éƒ¨")
    axes[0, 1].axis('off')
    
    axes[1, 1].imshow(original_patch, cmap='gray')
    axes[1, 1].set_title("é‡æ„å›¾åƒå±€éƒ¨")
    axes[1, 1].axis('off')
    
    # å·®å¼‚å›¾
    diff = np.abs(original_patch - reconstructed_patch)
    axes[0, 2].imshow(diff, cmap='jet')
    axes[0, 2].set_title("å·®å¼‚å›¾")
    axes[0, 2].axis('off')
    
    # éšè—æœ€åä¸€ä¸ªå­å›¾
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    return fig

# ä¸»åº”ç”¨
def main():
    st.set_page_config(
        page_title="ReconNet å¯è§†åŒ–æ¼”ç¤º",
        page_icon="ğŸ”„",
        layout="wide"
    )
    
    # åº”ç”¨æ ‡é¢˜
    st.title("ReconNet çº¢å¤–å›¾åƒå‹ç¼©ä¸é‡æ„å¯è§†åŒ–æ¼”ç¤º")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("æ¨¡å‹é…ç½®")
        
        # æ¨¡å‹åŠ è½½é€‰é¡¹
        model_path = st.text_input("ReconNetæ¨¡å‹è·¯å¾„", "Models/cs_reconnet_model.pth")
        
        # ä¸Šä¼ å›¾åƒ
        uploaded_file = st.file_uploader("ä¸Šä¼ çº¢å¤–å›¾åƒ", type=["jpg", "jpeg", "png"])
        
        # å¤„ç†é€‰é¡¹
        st.subheader("å¤„ç†å‚æ•°")
        show_details = st.checkbox("æ˜¾ç¤ºè¯¦ç»†å¤„ç†ä¿¡æ¯", True)
        show_performance = st.checkbox("æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡", True)
        
        # å…³äº
        st.markdown("---")
        st.subheader("å…³äº")
        st.info("""
        æœ¬åº”ç”¨æ¼”ç¤ºäº†åŸºäºæ ¼å¼å¡”ç†è®ºçš„çº¢å¤–å›¾åƒå‹ç¼©ä¸é‡æ„æŠ€æœ¯ã€‚
        é€šè¿‡è¯†åˆ«å›¾åƒä¸­çš„é‡è¦åŒºåŸŸï¼Œå®ç°é«˜æ•ˆçš„å›¾åƒå‹ç¼©ä¸é«˜è´¨é‡é‡æ„ã€‚
        """)
    
    # ä¸»å†…å®¹åŒº
    if uploaded_file is not None:
        # åŠ è½½æ¨¡å‹
        with st.spinner("åŠ è½½æ¨¡å‹ä¸­..."):
            try:
                # ä½¿ç”¨CSReconNetæ¨¡å‹
                reconnet_model = CSReconNet()
                try:
                    reconnet_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
                except FileNotFoundError:
                    st.warning(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
                reconnet_model.eval()
                
                # åŠ è½½Gestaltæ¨¡å‹
                gestalt_model = load_gestalt_model(ckpt_path='Models/optimized_gestalt_model_3.0.pth')
                gestalt_model.eval()
                
                st.success("æ¨¡å‹åŠ è½½æˆåŠŸ!")
            except Exception as e:
                st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                st.stop()
        
        # å¤„ç†ä¸Šä¼ çš„å›¾åƒ
        with st.spinner("å¤„ç†å›¾åƒä¸­..."):
            try:
                # è¯»å–å›¾åƒ
                image = Image.open(uploaded_file).convert('L')
                original_width, original_height = image.size
                
                # è°ƒæ•´å›¾åƒå¤§å°ä¸ºæ¨¡å‹è¾“å…¥å°ºå¯¸
                input_size = (640, 512)
                image_resized = image.resize(input_size)
                
                # è½¬æ¢ä¸ºå¼ é‡
                image_tensor = torch.tensor(np.array(image_resized), dtype=torch.float32)
                image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)
                image_tensor = (image_tensor / 255.0 - 0.5) / 0.5
                
                # é‡æ„å›¾åƒ
                result = reconstruct_image(reconnet_model, gestalt_model, image_tensor)
                
                # æ˜¾ç¤ºç»“æœ
                col1, col2 = st.columns(2)
                
                # åŸå§‹å›¾åƒå’Œé‡æ„å›¾åƒå¯¹æ¯”
                with col1:
                    st.subheader("åŸå§‹å›¾åƒä¸é‡æ„å›¾åƒå¯¹æ¯”")
                    fig_comparison = plot_image_comparison(
                        result['original_image'],
                        result['reconstructed_image'],
                        result['bbox_coords'],
                        "åŸå§‹å›¾åƒä¸é‡æ„å›¾åƒå¯¹æ¯”"
                    )
                    st.pyplot(fig_comparison)
                
                # åƒç´ çº§å¯¹æ¯”
                with col2:
                    st.subheader("åƒç´ çº§å¯¹æ¯”")
                    fig_pixel_comparison = plot_pixel_comparison(
                        result['original_image'],
                        result['reconstructed_image'],
                        result['bbox_coords']
                    )
                    st.pyplot(fig_pixel_comparison)
                
                # è¯„ä¼°æŒ‡æ ‡
                st.subheader("è¯„ä¼°æŒ‡æ ‡")
                metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
                
                with metrics_col1:
                    st.metric("å‹ç¼©ç‡", f"{result['compression_ratio']:.4f}")
                    st.metric("å¤„ç†æ—¶é—´", f"{result['processing_time']:.4f} ç§’")
                    st.metric("å³°å€¼ä¿¡å™ªæ¯”(PSNR)", f"{result['psnr']:.2f} dB")
                
                with metrics_col2:
                    st.metric("é‡è¦åŒºåŸŸMSE", f"{result['mse_non_important']:.6f}")
                    st.metric("éé‡è¦åŒºåŸŸMSE", f"{result['mse_important']:.6f}")
                    st.metric("ç»“æ„ç›¸ä¼¼æ€§(SSIM)", f"{result['ssim']:.4f}")
                
                with metrics_col3:
                    st.metric("å†…å­˜ä½¿ç”¨", f"{result['memory_usage']:.2f} MB")
                    st.metric("å›¾åƒå°ºå¯¸", f"{original_width}Ã—{original_height}")
                
                # è¯¦ç»†ä¿¡æ¯
                if show_details:
                    st.subheader("è¯¦ç»†å¤„ç†ä¿¡æ¯")
                    
                    x1, y1, x2, y2 = result['bbox_coords']
                    st.markdown(f"""
                    - **é‡è¦åŒºåŸŸåæ ‡**:
                      - å·¦ä¸Šè§’: ({x1}, {y1})
                      - å³ä¸‹è§’: ({x2}, {y2})
                      - å®½åº¦: {x2 - x1} åƒç´ 
                      - é«˜åº¦: {y2 - y1} åƒç´ 
                    """)
                    
                    st.markdown(f"""
                    - **åŸå§‹å›¾åƒå¤§å°**: {original_width}Ã—{original_height} åƒç´ 
                    - **å¤„ç†å›¾åƒå¤§å°**: {input_size[0]}Ã—{input_size[1]} åƒç´ 
                    """)
                
                # æ€§èƒ½æŒ‡æ ‡
                if show_performance:
                    st.subheader("æ€§èƒ½æŒ‡æ ‡")
                    
                    # è®¡ç®—å¹¶æ˜¾ç¤ºå‹ç¼©å‰åçš„å¤§å°
                    original_size_kb = result['original_num_elements'] / 8 / 1024
                    compressed_size_kb = result['compressed_num_elements'] / 8 / 1024
                    compression_percentage = result['compression_ratio'] * 100
                    savings_percentage = (1 - result['compression_ratio']) * 100
                    
                    st.markdown(f"""
                    - **åŸå§‹å›¾åƒå¤§å°**: {original_size_kb:.2f} KB ({result['original_num_elements']} æ¯”ç‰¹)
                    - **å‹ç¼©åæ•°æ®å¤§å°**: {compressed_size_kb:.2f} KB ({result['compressed_num_elements']} æ¯”ç‰¹)
                    - **å‹ç¼©ç‡**: {compression_percentage:.2f}%
                    - **å‹ç¼©èŠ‚çœç‡**: {savings_percentage:.2f}%
                    """)
                    
                    # ç»˜åˆ¶å‹ç¼©æ•ˆæœå›¾è¡¨
                    fig, ax = plt.subplots(figsize=(8, 4))
                    ax.bar(
                        ['åŸå§‹å›¾åƒ', 'å‹ç¼©åæ•°æ®'], 
                        [original_size_kb, compressed_size_kb],
                        color=['#1f77b4', '#ff7f0e']
                    )
                    ax.set_ylabel('æ•°æ®å¤§å° (KB)')
                    ax.set_title('å›¾åƒå‹ç¼©æ•ˆæœ')
                    ax.bar_label(ax.containers[0], fmt='%.2f')
                    st.pyplot(fig)
            
            except Exception as e:
                st.error(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
                # æ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                import traceback
                st.write(traceback.format_exc())
    else:
        # æœªä¸Šä¼ å›¾åƒæ—¶æ˜¾ç¤ºä»‹ç»
        st.markdown("""
        ### å…³äºæœ¬åº”ç”¨
        
        æœ¬åº”ç”¨å±•ç¤ºäº†åŸºäºæ ¼å¼å¡”ç†è®ºçš„çº¢å¤–å›¾åƒå‹ç¼©ä¸é‡æ„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡è¯†åˆ«å›¾åƒä¸­çš„é‡è¦åŒºåŸŸï¼Œå®ç°é«˜æ•ˆçš„å›¾åƒå‹ç¼©ä¸é«˜è´¨é‡é‡æ„ã€‚
        
        ### ä½¿ç”¨æ–¹æ³•
        
        1. åœ¨å·¦ä¾§ä¸Šä¼ ä¸€å¼ çº¢å¤–å›¾åƒ
        2. ç­‰å¾…ç³»ç»Ÿå¤„ç†å›¾åƒ
        3. æŸ¥çœ‹åŸå§‹å›¾åƒã€æ ‡æ³¨äº†é‡è¦åŒºåŸŸçš„å›¾åƒä»¥åŠé‡æ„åçš„å›¾åƒ
        4. æŸ¥çœ‹å„é¡¹è¯„ä¼°æŒ‡æ ‡
        
        ### æŠ€æœ¯ç‰¹ç‚¹
        
        - åŸºäºæ ¼å¼å¡”å¿ƒç†å­¦åŸç†è¯†åˆ«å›¾åƒä¸­çš„é‡è¦åŒºåŸŸ
        - å¯¹é‡è¦åŒºåŸŸå’Œéé‡è¦åŒºåŸŸé‡‡ç”¨ä¸åŒçš„å‹ç¼©ç­–ç•¥
        - åœ¨ä¿è¯é‡è¦ä¿¡æ¯å®Œæ•´æ€§çš„åŒæ—¶å®ç°é«˜æ•ˆå‹ç¼©
        - æä¾›ç›´è§‚çš„å¯è§†åŒ–ç•Œé¢ï¼Œä¾¿äºç†è§£å‹ç¼©æ•ˆæœ
        """)
        
if __name__ == "__main__":
    main()